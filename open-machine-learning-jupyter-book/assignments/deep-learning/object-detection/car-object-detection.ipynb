{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Object Detection\n",
    "\n",
    "# Problem Overview\n",
    "\n",
    "This notebook tackles **single object detection**. As the name suggests, single object detection entails the detection of a single object. More specifically, a model is tasked with providing the coordinates of the bounding box of an object in a particular image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "Since we are asked to predict a set of numbers (the coordinates of the bounding box of an object), we can treat single object detection as a linear regression problem, except with an image as input. To handle the image, we can utilise a convolutional neural network (CNN).\n",
    "\n",
    "Now that we have a fairly decent idea about how to approach this problem, we can get started with the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As usual, we'll start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization/Image Processing\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, Flatten, MaxPool2D, Dense\n",
    "\n",
    "# Other\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll setup our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/object-detection/archive.zip'\n",
    "\n",
    "r = requests.get(url)\n",
    "with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "\n",
    "train_path = Path(\"data/training_images\")\n",
    "test_path = Path(\"data/testing_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `cv2`, the library we'll be using to plot the bounding boxes and the images, only accepts integer values as vertices, we'll need to convert the coordinates of the bounding boxes to integers.\n",
    "\n",
    "The dataset contains information about multi-object detection; however, this notebook is concerned with single object detection. To account for this slight discrepancy, we will omit the duplicate values of the `image` column. This results in each image having only one corresponding set of bounding box coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_solution_bounding_boxes.csv\")\n",
    "train[['xmin', 'ymin', 'xmax', 'ymax']] = train[['xmin', 'ymin', 'xmax', 'ymax']].astype(int)\n",
    "train.drop_duplicates(subset='image', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll create some utility functions that make it easy to display images from files and dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img, bbox_coords=[], pred_coords=[], norm=False):\n",
    "    # if the image has been normalized, scale it up\n",
    "    if norm:\n",
    "        img *= 255.\n",
    "        img = img.astype(np.uint8)\n",
    "    \n",
    "    # Draw the bounding boxes\n",
    "    if len(bbox_coords) == 4:\n",
    "        xmin, ymin, xmax, ymax = bbox_coords\n",
    "        img = cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 3)\n",
    "        \n",
    "    if len(pred_coords) == 4:\n",
    "        xmin, ymin, xmax, ymax = pred_coords\n",
    "        img = cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 3)\n",
    "        \n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "def display_image_from_file(name, bbox_coords=[], path=train_path):\n",
    "    img = cv2.imread(str(path/name))\n",
    "    display_image(img, bbox_coords=bbox_coords)\n",
    "    \n",
    "def display_from_dataframe(row, path=train_path):\n",
    "    display_image_from_file(row['image'], bbox_coords=(row.xmin, row.ymin, row.xmax, row.ymax), path=path)\n",
    "    \n",
    "\n",
    "def display_grid(df=train, n_items=3):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # get 3 random entries and plot them in a 1x3 grid\n",
    "    rand_indices = [np.random.randint(0, df.shape[0]) for _ in range(n_items)]\n",
    "    \n",
    "    for pos, index in enumerate(rand_indices):\n",
    "        plt.subplot(1, n_items, pos + 1)\n",
    "        display_from_dataframe(df.loc[index, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A quick formatting note**: the green rectangle represents the correct bounding whereas the red rectangle represents the predicted bounding box. This convention is used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_file(\"vid_4_10520.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Data Generator\n",
    "\n",
    "Before training the model, we must define a generator that keras accepts. If you're not familiar with python generators or are in need of a quick refresher, check out [this resource](https://www.programiz.com/python-programming/generator).\n",
    "\n",
    "In keras, all we need to do is initialize some arrays containing images and their corresponding bounding box coordinates. Then, we simply return the newly-created arrays in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df=train, batch_size=16, path=train_path):\n",
    "    while True:        \n",
    "        images = np.zeros((batch_size, 380, 676, 3))\n",
    "        bounding_box_coords = np.zeros((batch_size, 4))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "                rand_index = np.random.randint(0, train.shape[0])\n",
    "                row = df.loc[rand_index, :]\n",
    "                images[i] = cv2.imread(str(train_path/row.image)) / 255.\n",
    "                bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])\n",
    "                \n",
    "        yield {'image': images}, {'coords': bounding_box_coords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary keys are crucial, as keras needs them to locate the correct input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "example, label = next(data_generator(batch_size=1))\n",
    "img = example['image'][0]\n",
    "bbox_coords = label['coords'][0]\n",
    "\n",
    "display_image(img, bbox_coords=bbox_coords, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "I'll use keras' functional API as it's incredibly easy to utilize custom inputs and predict custom outputs. Specifically, I'll use a fairly large neural network to start out with, and adjust the parameters of the layers if necessary.\n",
    "\n",
    "Notice that the dictionary keys in the generator correspond to the names of the input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=[380, 676, 3], name='image')\n",
    "\n",
    "x = input_\n",
    "\n",
    "for i in range(10):\n",
    "    n_filters = 2**(i + 3)\n",
    "    x = Conv2D(n_filters, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(2, padding='same')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(4, name='coords')(x)\n",
    "\n",
    "model = tf.keras.models.Model(input_, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we'll compile the model.\n",
    "\n",
    "For each output, we need to specify a loss and a metric. To do this, we simply reference the dictionary key used in the generator and assign it our desired loss function/metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss={\n",
    "        'coords': 'mse'\n",
    "    },\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    metrics={\n",
    "        'coords': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually train the model, let's define a callback that tests the current model on three, randomly selected images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to test the model. These will be called every epoch to display the current performance of the model\n",
    "def test_model(model, datagen):\n",
    "    example, label = next(datagen)\n",
    "    \n",
    "    X = example['image']\n",
    "    y = label['coords']\n",
    "    \n",
    "    pred_bbox = model.predict(X)[0]\n",
    "    \n",
    "    img = X[0]\n",
    "    gt_coords = y[0]\n",
    "    \n",
    "    display_image(img, pred_coords=pred_bbox, norm=True)\n",
    "\n",
    "def test(model):\n",
    "    datagen = data_generator(batch_size=1)\n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        test_model(model, datagen)    \n",
    "    plt.show()\n",
    "    \n",
    "class ShowTestImages(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        test(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll quickly use these methods to evaluate the current performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model isn't great; in fact, its predictions aren't even visible.\n",
    "\n",
    "But, the model's poor performance is expected as we haven't even trained the model yet.\n",
    "So, let's do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(\n",
    "    data_generator(),\n",
    "    epochs=9,\n",
    "    steps_per_epoch=500,\n",
    "    callbacks=[\n",
    "        ShowTestImages(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is doing quite well; the `MSE` is relatively low and the accuracy is very high.\n",
    "\n",
    "Since the model training seems to be complete, we can now export the model and store it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('car-object-detection.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Advay Patil](https://www.kaggle.com/advaypatil) for creating [Car Object Detection](https://www.kaggle.com/code/advaypatil/car-object-detection). It inspires the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
